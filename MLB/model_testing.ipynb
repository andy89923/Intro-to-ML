{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"model_testing.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"xy1XnX3vBWaZ"},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch import optim\n","from torchinfo import summary\n","import torchsummary\n","from torchvision import transforms\n","from video_dataset import VideoFrameDataset, ImglistToTensor\n","from pytorchtools import EarlyStopping\n","\n","from tqdm import tqdm\n","import random\n","import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yhgC1a7mDDep"},"source":["# Parameters\n","TEST_RATIO = 0.3\n","\n","MANUAL_SEED = 888\n","BATCH_SIZE = 4\n","NUMBER_OF_SEGMENTS = 8\n","\n","BATCH_SIZE = 32\n","EPOCHS = 30"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qrk_GoqMC6fc"},"source":["manualSeed = MANUAL_SEED\n","print(\"Random Seed: \", manualSeed)\n","random.seed(manualSeed)\n","torch.manual_seed(manualSeed)\n","torch.cuda.manual_seed(manualSeed)\n","torch.cuda.manual_seed_all(manualSeed)\n","np.random.seed(manualSeed)\n","\n","if torch.cuda.is_available():\n","    device = torch.device('cuda')\n","else:\n","    device = torch.device('cpu')\n","\n","print(f'Now running with device = {device}')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C4xL4W3WlRi8","executionInfo":{"status":"ok","timestamp":1638808797462,"user_tz":-480,"elapsed":396,"user":{"displayName":"陳琮方","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggv2H45-Mh5r26SNucBC8UgDDWujHpVy7p_W8dU=s64","userId":"14785561505396198489"}},"outputId":"1e0ed238-8b4a-4182-abc8-6e90a47c9751"},"source":["import random\n","\n","def split_dataset():\n","    dir = '/content/drive/MyDrive/Project/MLB/Data/Segment'\n","    dst_path = dir + '/all_test'\n","\n","    all = []\n","\n","    with open(f'{dst_path}/annotations.txt', 'r') as f:\n","        for item in f:\n","            all.append(item.strip())\n","\n","    n = int(len(all) * TEST_RATIO)\n","    test = random.sample(all, n)\n","    tran = []\n","    for i in all:\n","        if i not in test:\n","            tran.append(i)\n","\n","    print(f'Total dataset size = {len(all)}')\n","    print(f'==================================')\n","    print(f'Train size = {len(tran)}')\n","    print(f'Test  size = {len(test)}')\n","\n","    with open(f'{dst_path}/annotations_train.txt', 'w') as f:\n","        for item in tran:\n","            f.write(\"%s\\n\" % item)\n","\n","    with open(f'{dst_path}/annotations_test.txt', 'w') as f:\n","        for item in test:\n","            f.write(\"%s\\n\" % item)\n","\n","\n","split_dataset()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Total dataset size = 4456\n","==================================\n","Train size = 3120\n","Test  size = 1336\n"]}]},{"cell_type":"code","metadata":{"id":"VKNk-q9KDRuV"},"source":["preprocess = transforms.Compose([\n","        ImglistToTensor(),          # list of PIL images to (FRAMES x CHANNELS x HEIGHT x WIDTH) tensor\n","        transforms.Resize(299),     # image batch, resize smaller edge to 299\n","        transforms.CenterCrop(299), # image batch, center crop to square 299x299\n","        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Omsn-uuGGZwf"},"source":["dir = '/content/drive/MyDrive/Project/MLB/Data/Segment/all_test'\n","\n","train_dataset = VideoFrameDataset(\n","    root_path = dir,\n","    annotationfile_path = os.path.join(dir, \"annotations_train.txt\"),\n","    num_segments = NUMBER_OF_SEGMENTS,\n","    frames_per_segment = 1,\n","    imagefile_template='frame_{0:012d}.jpg',\n","    transform = preprocess,\n","    random_shift = True,\n","    test_mode = False\n",")\n","test_dataset = VideoFrameDataset(\n","    root_path = dir,\n","    annotationfile_path = os.path.join(dir, \"annotations_test.txt\"),\n","    num_segments = NUMBER_OF_SEGMENTS,\n","    frames_per_segment = 1,\n","    imagefile_template='frame_{0:012d}.jpg',\n","    transform = preprocess,\n","    random_shift = True,\n","    test_mode = False\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xvE5a0cYKGFv"},"source":[""],"execution_count":null,"outputs":[]}]}